{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOVE3osF7y6eVB7J2vc1j0e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZainAli24/Quater_3_online_classes/blob/main/Q3class4%2C5_online.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Topics that we are covered:**\n",
        "## **1 Agentic AI Stack**:\n",
        "![Agentic Ai stack](https://cdn.prod.website-files.com/66bb3d1f468f0f3848a20a84/67361c7879df5b88f88a2c7a_67361c696ebead1b0ac05d21_agents-stack-map-nov-14-24.webp)\n",
        "\n",
        "- **2  leaderboard**\n",
        "- **3  Generative AI Foundations**\n",
        "- **4  OpenAI Canvas tool**"
      ],
      "metadata": {
        "id": "6H4Z0Gku_TMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- # **Rag Concept:**:\n",
        ">- projector.tensorflow.org\n",
        "\n"
      ],
      "metadata": {
        "id": "grg1UqZERAmP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Class 5**:\n",
        ">- # **AG2: The Open-Source AgentOS**\n"
      ],
      "metadata": {
        "id": "Jxi2WzoCbfiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-pinecone langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkaP7hYAbeXU",
        "outputId": "3ccf78f5-d5ee-4012-b96e-e5b9d25d5e07"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.5/41.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.2 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/411.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('pinecone_database')\n",
        "pc = Pinecone(api_key=api_key)\n",
        "\n",
        "index_name = \"online-rag-class-7\"\n",
        "\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=768,       # jitni zyada dimension ki value ho gi ye utha hi deep ja ke vectors search karen ga,\n",
        "    metric=\"cosine\",     # lekin aghar dimension kam ho gi toh ye bhot detail mein nahe jae ga.\n",
        "    spec=ServerlessSpec(\n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "OEnNJDTUrHjW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "iko1B5MG2Plo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vectors = embeddings.embed_query(\"my name is ZAIN\")\n",
        "vectors[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gP0x0ptxy_bw",
        "outputId": "683aea93-4ed3-456a-a831-ea3e07b06526"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.025237170979380608,\n",
              " -0.021924424916505814,\n",
              " -0.025775693356990814,\n",
              " -0.06122611090540886,\n",
              " 0.037734854966402054]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "vector_store = PineconeVectorStore(\n",
        "    index=index,\n",
        "    embedding=embeddings\n",
        ")"
      ],
      "metadata": {
        "id": "oG5PIXiC7cQ-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"The top 10 soccer players in the world right now.\",\n",
        "    metadata={\"source\": \"website\"},\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
        "    metadata={\"source\": \"news\"},\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
        "    metadata={\"source\": \"tweet\"},\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "    document_4,\n",
        "    document_5,\n",
        "    document_6,\n",
        "    document_7,\n",
        "    document_8,\n",
        "    document_9,\n",
        "    document_10,\n",
        "]\n",
        "\n",
        "print(len(documents))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9WBWVw31EVl",
        "outputId": "8e94c079-5218-4d40-a6c7-d23288a8be78"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **UUID aur UUID4** :\n",
        "- **UUID** ek unique ID hoti hai, jo kisi bhi cheez ki alag pehchan ke liye use hoti hai.\n",
        "- **UUID4** ek specific tarika hai jo randomly unique IDs banata hai, yani har ID alag aur unique hoti hai.\n",
        "\n",
        " Yeh random IDs har dafa naye numbers aur letters ka combination banati hain, jo kisi aur ke sath match nahi karti.\n",
        "\n",
        "\n",
        "## **Relation :**\n",
        "- ### Loop ki har iteration ek nayi ID banata hai aur usse uuids list mein store karta hai.\n",
        "- ### Agar loop na ho, toh aap ek hi ID bana paoge, ya manually IDs banani padengi."
      ],
      "metadata": {
        "id": "NLHmpq3uIq69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "# Yahaan _ ka matlab hai \"loop ka index important nahi hai, bas loop chalana hai\".\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G8DH2F032cH",
        "outputId": "df4fd5ef-232b-40eb-a191-f2dcae1ff50b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cdb57a18-d562-43c6-8510-81989a112a6d',\n",
              " '1858ae37-a490-4cd9-a614-e21f27ca49dd',\n",
              " 'd29a706c-393b-4e46-bac2-d3893e37ad34',\n",
              " '34bd6e51-c613-493a-9938-e350092ff58c',\n",
              " '4453ac50-7993-4f11-8a06-d2531d5c6e07',\n",
              " 'bc6ea185-0bf4-4fe7-bc42-ec82cb975b7a',\n",
              " '40c8b824-00ea-4fc2-be28-6ad6aca1f015',\n",
              " '18245cce-6d45-4033-ac10-3eeb2d63f04a',\n",
              " 'ffd40918-f3aa-48ff-a9d3-615caf5207fb',\n",
              " 'ec56ccf3-2b6a-40f0-a911-57408b2df2d4']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note\n",
        "- ### **similarity_search** : Yeh function query ke base par vector store ke andar similar documents dhoondhta hai.\n",
        "\n",
        "- ### **k=2** : Yeh batata hai ki sirf top 2 most similar documents return karne hain.\n",
        "\n",
        "- ### **filter**={\"source\": \"tweet\"}: Yeh ek condition hai ki sirf wahi documents search karo jinka \"source\" \"tweet\" ho."
      ],
      "metadata": {
        "id": "TR6ZEh9_utjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search(\n",
        "    \"LangChain provides abstractions to make working with LLMs easy\",\n",
        "    k=2,\n",
        "    filter={\"source\": \"tweet\"}\n",
        ")\n",
        "\n",
        "for res in results:\n",
        "  print(f\"* {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM2NJFoVrmM1",
        "outputId": "9c74fd2e-b245-4cb5-9401-fc83f10ccb47"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* LangGraph is the best framework for building stateful, agentic applications! [{'source': 'tweet'}]\n",
            "* Building an exciting new project with LangChain - come check it out! [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **similarity_search_with_score:**\n",
        " - Yeh function query ke liye similar documents search karta hai aur saath mein unka similarity score bhi deta hai.\n",
        " - Similarity score: Ek number jo dikhata hai ki document query se kitna match karta hai. Score jitna zyada, utna zyada relevant.\n",
        "\n",
        "\n",
        "### **score:3f :**  is ka matlab hai point ke baad 3 digits tak ki value do , aur baaki ko round off kar diya jayega.\n",
        "- Float wo numbers hote hain jo point (.) ke sath aate hain.\n",
        "\n"
      ],
      "metadata": {
        "id": "MDPcSmKTzUXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search_with_score(\n",
        "    \"Will it be hot tomorrow?\",\n",
        ")\n",
        "for res, score in results:\n",
        "    print(f\"* [SIM={score:.3f}] {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BaFoD1hyY1g",
        "outputId": "601cca85-38b4-426e-aaf6-3051a135c5fe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* [SIM=0.668] The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees. [{'source': 'news'}]\n",
            "* [SIM=0.577] I have a bad feeling I am going to get deleted :( [{'source': 'tweet'}]\n",
            "* [SIM=0.538] I had chocalate chip pancakes and scrambled eggs for breakfast this morning. [{'source': 'tweet'}]\n",
            "* [SIM=0.534] The stock market is down 500 points today due to fears of a recession. [{'source': 'news'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "QS8D22Yon3Xa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    api_key=api_key,\n",
        "    temperature=0.9,\n",
        "    model=\"gemini-1.5-flash\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Q-MQlvORnkCt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def anwer_query(query:str):\n",
        "  vector_results = vector_store.similarity_search(query , k=2)\n",
        "  final_asnwer = llm.invoke(f\"ANSWER THIS USER QUERY: {query}, Here are some reference to answer {vector_results}\")\n",
        "  return final_asnwer"
      ],
      "metadata": {
        "id": "tIG7yGbwrBo8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = anwer_query(\"LangChain provides abstractions to make working with LLMs easy\")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "EjgNHbwPsetU",
        "outputId": "17f16cd9-55fe-40ce-c740-a530bee9455f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the provided documents, LangChain is used for building applications, specifically  an \"exciting new project\" according to one tweet.  The tweets don\\'t directly compare LangChain to LangGraph, but one tweet highlights LangGraph\\'s strengths in building \"stateful, agentic applications\".  Therefore, while LangChain is used for application building, the provided text suggests LangGraph might be a better choice for applications requiring statefulness and agency.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}